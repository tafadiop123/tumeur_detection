# -*- coding: utf-8 -*-
"""Détection et Localisation de Tumeur Cérébrale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Rg4jfMPjdK77E0DwMtskbUMMEkUCsUo

# 1 - COMPRENDRE L'ETAT DU PROBLEME ET L'ETUDE DE CAS

* L'Intelligence Artificielle est entrain de révolutionner la Santé dans plusieurs domaines par exemple : 

  * Diagnostic de maladie par imagerie médicale
  * Robots de chirurgie
  * Maximisation de l'efficacité des hôpitaux

* Le marché de l'IA pour la santé prévoit d'atteindre 45.2 miliards de dollars en 2026 pour une valeur actuelle de 4.9 milliards de dollars   

* Le Deep learning a été prouvé qu'il est supérieur dans la détection des maladies à partir d'images de scanners IRM, CT (Computerized Tomography) et de X_rays, ce qui pourrait améliorer significativement la rapidité et l'exactitude des diagnostics.

  * La tomodensitométrie (TDM), dite aussi scanographie, tomographie axiale calculée par ordinateur (TACO), CT-scan (CT : computerized tomography), CAT-scan (CAT : computer-assisted tomography), ou simplement scanner ou scanneur pour l'appareil, est une technique d'imagerie médicale qui consiste à mesurer l'absorption des rayons X par les tissus puis, par traitement informatique, à numériser et enfin reconstruire des images 2D ou 3D des structures anatomiques. Pour acquérir les données, on emploie la technique d'analyse tomographique ou « par coupes », en soumettant le patient au balayage d'un faisceau de rayons X.

* Dans cette étude de cas, on va considérer que l'on travaille en tant que Consultant AI/ML et qu'on est embauché une compagnie de diagnostic médicale basée à New York

* Mon rôle est d'améliorer la rapidité et l'exactitude de la détection et de de la localisation de tumeur cérébrale en basant sur des images d'IRM

* Cela pourrait réduire drastiquement le coût de diagnostic du cancer et pourrait aider à diagnostiquer plus tôt les cas de tumeurs ce qui serait un sauveteur de vie.

* On considère que l'équipe a collecté des images d'IRM de cerveaux et ils se sont rapprochés de moi pour le développement d'un modèle capable de détecter et de localiser les tumeurs.

* Ci-dessous un exemple de détection et de localisation de tumeurs cérébrales à partir d'images d'IRM :  


* Source de Données: https://www.kaggle.com/mateuszbuda/lgg-mri-segmentation
* Lien d'une liste complète de startups dans l'imagerie médicale avec l'IA: https://research.aimultiple.com/looking-for-better-medical-imaging-for-early-diagnostic-and-monitoring-contact-the-leading-vendors-here/

**---> Un pipeline d'apprentissage en profondeur par couches pour effectuer la Segmentation et la Classification**


**---> Qu'est-ce-que la Segmentation d'images ?**

* Le but de la segmentation d'images est de comprendre et d'extraire des informations à partir des images au niveau des pixels.

* La segmentation d'image peut être utilisé pour la reconnaisance et la localisation d'objets qui offrent une valeur importante dans plusieurs applications comme l'imagerie médicale et les voitures autonomes, etcc.

* Le but de la segmenttion est d'entrainer un réseau de neurones afin de produire un masque de l'image au pixel près.

*  Les techniques modernes de segmentation d'images sont basées sur une approche d'apprentissage profond ce qui fait appel à des architectures communes comme les réseaux de neurones convolutionnels (CNN), les FCNs (Fully Convolution Networks) (Réseaux à Convolution Totale) et les "Deep Encoders-Decoders"

* On va utiliser l'architecture "ResUNet" pour résoudre le problème posé : 

* Dans le cas du Unet, on converti (encode) l'image en un vecteur et ensuite on le décode par un échantillonage en image. 

* Dans le cas d' "Unet", l'entrée et la sortie ont la même taille alors la taille de l'image est préservée .

* **Les CNN classiques** : ils sont généralement utilisés quand l'image complète est nécessaire

* Pour le **"Unet"**: La classification au niveau pixel est effectuée

* **U-net** formule un fonction de perte (***loss function***) pour chaque pixel de l'image d'entrée

* La fonction ***Softmax*** est appliquée à chaque pixel qui fait que le problème de segmentation marche comme un problème de classification quand la classification est efectueé sur chaque pixel de l'image.

* *! Article intéressant à voir : https://towardsdatascience.com/introduction-to-u-net-and-res-net-for-image-segmentation-9afcb432ee2f

# 2  - IMPORTER LES LIBRAIRIES ET LES DATASETS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import zipfile
import cv2
from skimage import io
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
from tensorflow.keras import backend as K
from sklearn.preprocessing import StandardScaler, normalize
import os
import glob
import random
from google.colab import files #library to upload files to colab notebook
# %matplotlib inline

# On va monter le drive 
# For more information regarding mounting, please check this out: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Naviguer au chemin des datasets
# %cd /content/drive/My Drive/Colab Notebooks/Modern Artificial Intelligence Masterclass UDEMY/Healthcare_AI_Brain_Tumor_Detection

#dataset_Dr = "./Healthcare AI Datasets/Brain_MRI/"

# Les données contenant le chemin vers les IRM du cerveau et leurs masques correspondants 
brain_df = pd.read_csv("data_mask.csv")

"""
## **---> MASK (MASQUE):**

* Le but de la segmentation d'image est de comprendre l'image au niveau des pixels avec une certaine classe. Le résultat produit par le modèle de la segmentation d'image est appelé "mask" (masque) de l'image

* Les "Masks" peuvent être représentés en associant les valeurs de pixels avec leurs coordonnées. Par exemple si nous une image noire de dimensions (2,2), il peut être représenté comme :


* Pour représenter ce "mask" on doit d'abord applatir l'image en une matrice D1 (1 Dimension). Ce qui pourrait donner une matrice comme : **[255, 0, 0, 255]** pour le "mask". Alors on peut utiliser l'index pour créer le "mask". Finallement on voudrait avoir quelque chose comme **[1, 0, 0, 1]** en tant que notre "mask" 
"""

brain_df.info()

brain_df.head(10)

brain_df.mask_path[1] #Le chemin vers le 2ème masque de segmentation

brain_df.image_path[1] #Le chemin vers la 2ème image de l'IRM du cerveau

"""**MINI CHALLENGE**

* Vérifier si on a un dataset non balancé ou pas

* Combien d'individus sont classés comme en bonne santé ?

"""

brain_df["mask"].value_counts()

"""**!!! 1 - J'ai un dataset non balancé**

**!!! 2 -  Il y'a** *2556* **patients qui sont classés en bonne santé.**

# 3 - Faire la visualisation des données
"""

brain_df

# Regarder les variables cibles 
brain_df["mask"].value_counts().index

# Utiliser Plotly pour visualiser le nombres de patients qui sont malades et qui le sont pas
import plotly.graph_objects as go

fig = go.Figure([go.Bar(x = brain_df["mask"].value_counts().index, y = brain_df["mask"].value_counts())])
fig.update_traces(marker_color = "rgb(0,255,0)", marker_line_color = "rgb(255,0,0)", marker_line_width = 3, opacity = 0.6)
fig.show()

#Les chemins vers les différents images de "mask"
brain_df.mask_path

# Les différents images d'IRM
brain_df.image_path

# Visualisation graphique du 623ème image mask
plt.imshow(cv2.imread(dataset_Dr+brain_df.mask_path[623]))

# Visualisation graphique de la 623ème image d'IRM
plt.imshow(cv2.imread(dataset_Dr+brain_df.image_path[623]))

# La valeur maximale de la couleur présente dans la 623ème image mask
cv2.imread(dataset_Dr+brain_df.mask_path[623]).max()

# La valeur minimale de la couleur présente dans la 623ème image mask
cv2.imread(dataset_Dr+brain_df.mask_path[623]).min()

# Visualisation des images (IRM et Mask) 
import random

fig, axs = plt.subplots(6, 2, figsize=(16,32))
count = 0
for x in range(6):
  i = random.randint(0, len(brain_df)) #selectionner un index aléatoire
  axs[count][0].title.set_text("IRM du Cerveau") #mettre le titre
  axs[count][0].imshow(cv2.imread(dataset_Dr+brain_df.image_path[i])) #représenter l'IRM
  axs[count][1].title.set_text("Mask - " + str(brain_df["mask"][i])) #mettre le titre
  axs[count][1].imshow(cv2.imread(dataset_Dr+brain_df.mask_path[i])) #représenter le mask correspondant
  count += 1

fig.tight_layout()

"""**MINI CHALLENGE**

* Représenter aléatoirement 12 images d'IRM sélectionnées à partir seulement des patients atteints de cancer, 
* Suivi du mask correspondant 
* Et enfin les 2 images dont l'image d'IRM et son masque correspondant au-dessus d'elle en couleur rouge
"""

# Visualisation des images (IRM et de leurs Mask correspondants) 

count = 0
fig, axs = plt.subplots(12, 3, figsize=(20,50))
for i in range(len(brain_df)):
  if brain_df["mask"][i] ==1 and count <12:
    img = io.imread(dataset_Dr+brain_df.image_path[i])
    axs[count][0].title.set_text("IRM du Cerveau")
    axs[count][0].imshow(img)
    
    mask = io.imread(dataset_Dr+brain_df.mask_path[i])
    axs[count][1].title.set_text("Mask")
    axs[count][1].imshow(mask, cmap = "gray")

    img[mask == 255] = (255,0,0) # Si j'ai une pixel blanche dans le mask, je remplace cette valeur de pixel par la couleur rouge
    axs[count][2].title.set_text("IRM du Cerveau et Mask")
    axs[count][2].imshow(img)
    count += 1
    
fig.tight_layout()

"""# 4 - Comprendre La Theorie et l'Intuition Derrière les Réseaux de Neurones Convolutionnelles (**CNN**) et les RESNETS (**REVIEW**)

* Les premiers layers du CNN sont utilisées pour extraire des caractéristiques générales de haut niveau

* Les deux dernières couches (layers) du CNN sont utilisées pour faire la classification (sur des tâches spécifiques)

* Les champs respectifs locals scanne l'image en premier cherchant des formes comme des bords et des lignes

* Ces bords sont alors récupérés par la couche subséquente pour former des caractéristiques encore plus complexes. 



- Convolutions et Extraction des Features : https://setosa.io/ev/image-kernels/
-  Visualization des CNN : https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html

# RESNET (RESIDUAL NETWORK)(REVIEW)

* Plus les CNN s'agrandissent en profondeur, la disparition ou la descente du gradient tend à se produire ce qui impacte négativement la performance du réseau car le gradient sera tellement petit qu'il pourra plus mettre à jour les poids du réseau neuronal 

* Le problème de la descente du gradient se produit quand le gradient est rétro-propagé (back-propagated) aux layers (couches)  précédentes, ce qui se traduit par une très faible pente.

* Le "Residual Neural Network" inclu la fonctionnalité "skip connection" qui permet l'entrainement de 152 layers sans problème de disparition du gradient.

* Le RESNET marche en ajoutant du "identity mappings" en haut du CNN.

* "ImageNet" est une base de donnéees qui contient 11 millions d'images dont 11000 catégories.

* Cette base de donnée est utilisée pour entrainer le réseau neuronal profond "ResNet"



* Lien vers le document intitulé **RESNET** (Deep Residual Learning for Image Recognition): https://arxiv.org/pdf/1512.03385.pdf

# 5 - Comprendre la Théorie et l'Intuition derrière le "Transfert Learning" (Transfert d'Apprentissage)

* Le trnasfert d'apprentissage est une technique de machine learning dans lequel un réseau est entrainé afin d'effectuer une tâche spécifique qui sera réutilisée (reconduite) vers un point de départ d'une autre tâche similaire.

* Le transfert learning est largement utilisé depuis le début des modèles pré-entrainés qui peuvent considérablement réduire le temps de calcul requis si l'entrainement est fait à partir de rien.


## Le Processus du Transfert d'Apprentissage (TRANSFERT LEARNING)

Ce que l'on fait en général, c'est que l'on prend un réseau neurones convolutionel déja entrainé sur le dataset "***ImageNET***", ensuite on conserve les couches d'extraction des caractérisques autrement appelé les couches convolutionels puis on va transférer ces paramètres entrainés sur de nouvelles images (dans notre cas on a les images d'IRM du cerveau) et enfin on va y ajouter un nouvelles couches "dense" (compact) que l'on va entrainer sur des tâches spécifiques.

En résumé, on va geler les couches convolutionelles pré-entrainées contenant les caractéristiques transférées et juste entrainer la nouvelle couche afin de déterminer par exemple si le patient a une tumeur cérébrale ou non.



# 6 - Entrainer le Modèle de Classification pour Détecter Si la Tumeur Existe ou Non
"""

# On enlève la colonne de l'ID du Patient
brain_df_train = brain_df.drop(columns = ["patient_id"])
brain_df_train.shape

# On converti les données de la colonne "mask" en format string, pour utiliser le mode "categorical" 
# dans la fonction "flow_from_dataframe". Dans le cas contraire on aurait un message du type :
# TypeError: IF class_mode="categorical", y_col="mask" columns values must be string, list or tuple.

brain_df_train["mask"] = brain_df_train["mask"].apply(lambda x: str(x))
brain_df_train.info()

# Splitter les données en données d'entrainement et de test
from sklearn.model_selection import train_test_split

train, test = train_test_split(brain_df_train, test_size=0.15)

# Créer un générateur d'image
from keras_preprocessing.image import ImageDataGenerator

#Créer un générateur de données qui régle les valeurs de pixels entre 0 et 1 et faire ressortir un échantillon de 15% pour la validation
datagen = ImageDataGenerator(rescale = 1./255., validation_split = 0.15)

train_generator = datagen.flow_from_dataframe(
    dataframe = train,
    directory = dataset_Dr,
    x_col = "image_path",
    y_col = "mask",
    subset = "training",
    batch_size = 16,
    shuffle = True,
    class_mode = "categorical",
    target_size = (256, 256)
)
valid_generator = datagen.flow_from_dataframe(
    dataframe = train,
    directory = dataset_Dr,
    x_col = "image_path",
    y_col = "mask",
    subset = "validation",
    batch_size = 16,
    shuffle = True,
    class_mode = "categorical",
    target_size = (256,256)
)

# Créer un générateur de données pour les données de test
test_datagen = ImageDataGenerator(rescale=1./255.)

test_generator = test_datagen.flow_from_dataframe(
    dataframe = test,
    directory = dataset_Dr,
    x_col = "image_path",
    y_col = "mask",
    batch_size = 16,
    shuffle = False,
    class_mode = "categorical",
    target_size = (256,256)
)

# Prendre la base du model c'est-à-dire les premières couches convolutionelles
basemodel = ResNet50(weights="imagenet", include_top=False, input_tensor=Input(shape=(256, 256, 3)))

# Architecture des couches de base 
basemodel.summary()

# Ensuite on géle les couches du modèle pré-entrainé
for layer in basemodel.layers:
  layers.trainable = False

# Ajouter une couche compacte à la base du modèle
headmodel = basemodel.output
headmodel= AveragePooling2D(pool_size = (4,4))(headmodel)
headmodel = Flatten(name="flatten")(headmodel)
headmodel = Dense(256, activation="relu")(headmodel)
headmodel = Dropout(0.3)(headmodel)
headmodel = Dense(256, activation="relu")(headmodel)
headmodel = Dropout(0.3)(headmodel)
headmodel = Dense(2, activation="softmax")(headmodel)

model = Model(inputs=basemodel.input, outputs = headmodel)

model.summary()

# Compiler le modèle
model.compile(loss = "categorical_crossentropy", optimizer="adam", metrics= ["accuracy"])

# Utiliser la classe "EarlyStopping" pour quitter l'entranement lorsque le "loss" de la validition arrête
# de diminuer aprés un certains nombres d'epochs
earlystopping = EarlyStopping(monitor="val_loss", mode ="min", verbose = 1, patience=20)

# Energistrer les meilleurs poids du modèle avec moins de d'erreurs de validation
checkpointer = ModelCheckpoint(filepath="classifier-resnet-weights.hdf5", verbose = 1, save_best_only = True)

history = model.fit(train_generator, steps_per_epoch = train_generator.n // 16, epochs = 100, validation_data = valid_generator, validation_steps = valid_generator.n // 16, callbacks = [checkpointer, earlystopping])

# Enregistrer l'architecturure du modèle dans un fichier JSON pour une utilisation future

model_json = model.to_json()
with open("classifier-resnet-model.json", "w") as json_file:
  json_file.write(model_json)

"""**MINI CHALLENGE**

* Changer l'architecture du modèle en ajoutant plus/moins de couches compactes(dense), de neurones et de dropout

* Afficher le résumé du modèle et comparer le nombres totals de paramètres entrainables entre l'original et le nouveau modèle
 

"""



"""# 7 - Evaluer les Performances du Modèle Entrainé 


"""

# Charger le modèle pré-entrainé 
with open("classifier-resnet-model.json", "r") as json_file:
  json_savedModel = json_file.read()

# Charger le modèle
model = tf.keras.models.model_from_json(json_savedModel)
model.load_weights("classifier-resnet-weights.hdf5")
model.compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = ["accuracy"])

# Faire les prédictions
test_predict = model.predict(test_generator, steps = test_generator.n // 16, verbose = 1)

test_predict.shape

test_predict

# Obtenir les classes prédites à partir du modèle de prédiction
predict = []

for i in test_predict:
  predict.append(str(np.argmax(i)))

predict = np.asarray(predict)

predict

# Depuis qu'on a utiliser le "test generator", il limite le nombre d'images au len(predict) à cause de la taille du batch
original = np.asarray(test["mask"])[:len(predict)]
len(original)

# Obtenir l'exactitude du modèle
from sklearn.metrics import  accuracy_score

accuracy = accuracy_score(original, predict)
accuracy

# Représentation graphique de la Matrice de Confusion
from sklearn.metrics import  confusion_matrix

cm = confusion_matrix(original, predict)
plt.figure(figsize = (7,7))
sns.heatmap(cm, annot=True)

"""# 8 - Comprendre l'Intuition du Modèle de Segmentation ***RESUNET*** 

**---> RESUNET**

* L'architecture du "ResUNet" combine l'architecture de base du "UNet" avec les blocks résiduels (**residual blocks**)  afin de surmonter le problème de la disparition des gradients présente dans les architectures profondes. 

* L'architecture "UNet" est basé sur des réseaux totalement convolutifs et modifié dans le but qqu'il puisse effectuer parfaitement les tâches de segmentation 

* Le ResUNet est composé de 3 parties : 

    * 1) Le chemin encodeur ou contractant
    * 2) Le "BottleNeck" qui forme en V de l'architecture du modèle, d'où son nom de "ResUNet"
    * 3) Le chemin décodeur ou élargissant


### **RESUME DE L'ARCHITECTURE DU RESUNET**

1. Le Chemin Encodeur ou Contractant comporte 4 blocs:

* Le premier bloc se compose d'une Couche Convolutionelle 3x3 + une "Batch Normalization" + une fonction d'activation "Relu"

* Les 3 blocs restants se composent d'un bloc résiduel suivi par 2 "Max-Pooling" 2x2

2. BottleNeck

* Il se trouve au milieu du chemin contractant et élargissant 

* Il se compose d'un bloc résiduel suivi d'une Couche Convolutionelle "Up-Sampling" 2x2

3. Le Chemin Décodeur ou Elargissant comporte 4 blocs:

* Les 3 blocs qui suivent le BottleNeck se composent de blocs résiduel suivi par 2 couches convolutionnelles "up-sampling" 2x2

* Le dernier bloc se compose d'un bloc résiduel suivi par une couche convolutionnelle 1x1.

# 9 - Construire Un Modèle de Segmentation pour la Localisation de la Tumeur Cérébrale
"""

# Obtenir le DataFrame contenant les images d'IRM avec leurs masks associés à elles.
brain_df_mask = brain_df[brain_df["mask"] == 1]
brain_df_mask.shape

brain_df_mask["image_path"]

brain_df_mask["mask_path"]

# Splitter les données en données d'entrainement et de test 

from sklearn.model_selection import train_test_split

X_train, X_val = train_test_split(brain_df_mask, test_size=0.15)
X_test, X_val = train_test_split(X_val, test_size=0.5)

# Créer une liste séparée d'ID des images (imageId) et d'ID des classe (classId) qui va passer dans le generateur de données

train_ids = list(X_train.image_path)
train_mask = list(X_train.mask_path)

val_ids = list(X_val.image_path)
val_mask = list(X_val.mask_path)

# Charger le fichier "Utilities" qui contient le générateur de données personnalisé et aussi la fonction "loss" personnalisée
from utilities import DataGenerator

# Créer les générateurs de données

training_generator = DataGenerator(train_ids, train_mask)
validation_generator = DataGenerator(val_ids, val_mask)

# Le Bloc Résiduel (Res-Block)
def res_block(X, f):
  # Faire une copie de l'input
  X_copy = X
  #Chemin Principal (Main Path)
  #
  X = Conv2D(f, kernel_size=(1,1), strides=(1,1), kernel_initializer="he_normal")(X)
  X = BatchNormalization()(X)
  X = Activation("relu")(X)

  X = Conv2D(f, kernel_size=(3,3), strides=(1,1), padding="same", kernel_initializer="he_normal")(X)
  X = BatchNormalization()(X)

  # Short Path
  #
  X_copy = Conv2D(f, kernel_size=(1,1), strides=(1,1), kernel_initializer="he_normal")(X_copy)
  X_copy = BatchNormalization()(X_copy)

  # Ajouter le résultat du main path et du short path ensemble
  X = Add()([X,X_copy])
  X = Activation("relu")(X)

  return X

# La fonction qui permet d'agrandir et de concaténer les valeurs passées
def upsample_concat(x, skip):
  x = UpSampling2D((2,2))(x)
  merge = Concatenate()([x, skip])

  return merge

# Le réseau de Neurones Entier
input_shape = (256, 256, 3)

# La dimension du Tenseur d'entrée
X_input = Input(input_shape)

# Stage 1 
conv1_in = Conv2D(16, 3, activation="relu", padding="same", kernel_initializer="he_normal")(X_input)
conv1_in = BatchNormalization()(conv1_in)
conv1_in = Conv2D(16, 3, activation="relu", padding="same", kernel_initializer="he_normal")(conv1_in)
conv1_in = BatchNormalization()(conv1_in)
pool_1 = MaxPool2D(pool_size=(2,2))(conv1_in)

# Stage 2 
conv2_in = res_block(pool_1, 32)
pool_2 = MaxPool2D(pool_size=(2,2))(conv2_in)

# Stage 3
conv3_in = res_block(pool_2, 64)
pool_3 = MaxPool2D(pool_size=(2,2))(conv3_in)

# Stage 4
conv4_in = res_block(pool_3, 128)
pool_4 = MaxPool2D(pool_size=(2,2))(conv4_in)

# Stage 5 (BottleNeck)
conv5_in = res_block(pool_4, 256)

# Upscale stage 1 
up_1 = upsample_concat(conv5_in, conv4_in)
up_1 = res_block(up_1, 128)

# Upscale stage 2
up_2 = upsample_concat(up_1, conv3_in)
up_2 = res_block(up_2, 64)

# Upscale stage 3
up_3 = upsample_concat(up_2, conv2_in)
up_3 = res_block(up_3, 32)

# Upscale stage 3
up_4 = upsample_concat(up_3, conv1_in)
up_4 = res_block(up_4, 16)

# Le Résultat Final
output = Conv2D(1, (1,1), padding="same", activation="sigmoid")(up_4)

model_seg = Model(inputs = X_input, outputs = output)

# Résumé du modèle de réseau de neurone pour la segmentation 
model_seg.summary()

"""# 10 - Entrainer Le Modèle de Segmentation RESUNET pour la Localisation de Tumeur Cérébrale

### **La Fonction LOSS**
On a besoin d'une fonction "loss" personnalisée pour entrainer ce ResUNet, Donc, on va utiliser la fonction loss à partir de :  https://github.com/nabsabraham/focal-tversky-unet/blob/master/losses.py
"""

from utilities import  focal_tversky, tversky_loss, tversky

# Compiler le modèle
adam = tf.keras.optimizers.Adam(lr=0.05, epsilon=0.1)
model_seg.compile(optimizer=adam, loss=focal_tversky, metrics=[tversky])

# Utiliser le "EarlyStopping" afin de sortir de l'entrainement si le loss de validation n'est pas entrain de descendre après un certains nombres d'époches (patience)
earlystopping = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=20)

# Enregistrer le meilleur modèle avec le plus petit loss de validation
checkpointer = ModelCheckpoint(filepath="ResUNet-weights.hdf5", verbose=1, save_best_only=True)

#Entrainement du modèle 
history = model_seg.fit(training_generator, epochs=100, validation_data=validation_generator, callbacks=[checkpointer, earlystopping])

# Enregistrer l'architecture du modèle dans un fichier JSON pour une utilisation future

model_json = model_seg.to_json()
with open('ResUNet-model.json', 'w') as json_file:
  json_file.write(model_json)

"""# 11 - Evaluer les Performances du Modèle de Segmentation **RESUNET** Entrainé 

"""

from utilities import focal_tversky, tversky_loss, tversky

with open('ResUNet-model.json', 'r') as json_file:
  savedModel_json = json_file.read()

# Charger l'architecture du Modèle 
model_seg = tf.keras.models.model_from_json(savedModel_json)
model_seg.load_weights("ResUNet-weights.hdf5")
adam = tf.keras.optimizers.Adam(lr=0.05, epsilon=0.1)
model_seg.compile(optimizer=adam, loss=focal_tversky, metrics=[tversky])

# Charger le fichier python "Utilities" qui contient les fonctions "loss" et générateur de données personnalisés

from utilities import  prediction

# Faire les predictions sur de nouvelles images en faisant de l'inférence sur les 2 modèles déja construit 
# A savoir d'abord détecter la tumeur si elle existe ou pas, et ensuite faire la localisation de la tumeur à travers le modèle (**model_seg**)
image_id, mask, has_mask = prediction(X_test, model, model_seg)

# Créer un Dataframe pour contenir les résultats de le prédiction sur les nouvelles images
df_pred = pd.DataFrame({'image_path': image_id, 'predicted_mask' : mask, 'has_mask' : has_mask})
df_pred

# Merger les 2 dataframes dont le dataframe contenant les résultats prédits avec les images tests originales
df_pred = X_test.merge(df_pred, on="image_path")
df_pred.head()

# Le code final qui permet de faire la représentation des images avec la prédiction du modèle de détection et celle de localisation
# de la tumeur cérébrale. 

count = 0
fig, axs = plt.subplots(10, 5, figsize=(30, 40))
for i in range(len(df_pred)):
  if df_pred["has_mask"][i] == 1 and count < 10:
    # Lire les images et les convertir au format rgb
    img = io.imread(df_pred.image_path[i])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    axs[count][0].title.set_text("IRM du Cerveau")
    axs[count][0].imshow(img)

    # Obtenir le mask original des images
    mask = io.imread(df_pred.mask_path[i])
    axs[count][1].title.set_text("Le Masque Original")
    axs[count][1].imshow(mask)

    # Obtenir les masks prédits pour les images
    predicted_mask = np.asarray(df_pred.predicted_mask[i])[0].squeeze().round()
    axs[count][2].title.set_text("Le Masque Prédit par l'IA")
    axs[count][2].imshow(predicted_mask)

    # Appliquer le masque sur l'image en transformant la partie blanche en rouge 
    img[mask == 255] = (255, 0, 0)
    axs[count][3].title.set_text("Le Masque Original associé à l'IRM du Cerveau")
    axs[count][3].imshow(img)

    # Appliquer le masque sur l'image en transformant la partie blanche en vert
    img_ = io.imread(df_pred.image_path[i])
    img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)
    img_[predicted_mask == 1] = (0, 255, 0)
    axs[count][4].title.set_text("Le Masque Prédit associé à l'IRM du Cerveau")
    axs[count][4].imshow(img_)
    count += 1

fig.tight_layout()

